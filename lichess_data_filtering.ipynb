{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wME41U5kSm6Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JooX1hESm6Z"
      },
      "source": [
        "Our data begins as a bunch of PGN transcripts. However, to work in tensors we need all transcripts to be the same length. So, this file takes our PGNs and performs some filtering.\n",
        "\n",
        "This notebook has a very similar counterpart, `utils\\chess_gpt_eval_data_filtering.ipynb`. The lichess and chess_gpt_eval datasets have a different structure and different column names. For most peoples' needs, the lichess dataset alone should suffice, so I made two separate notebooks to keep this one simple.\n",
        "\n",
        "The output of this file is 4 different csv's:\n",
        "\n",
        "`lichess_100mb.csv`\" 100 MB of lichess PGN games, with every game also containing player Elo information.\n",
        "\n",
        "`lichess_100mb_filtered.csv`: We perform some filtering for game length, add player Elo bucket, and do some manipulation of the PGN string.\n",
        "\n",
        "`lichess_train.csv` and `lichess_test.csv` a 50 / 50 train / test split of `lichess_100mb_filtered.csv`, used for training and testing linear probes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGiAMwDhSm6b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmZDJvd4Sm6b"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"data/\"\n",
        "prefix = \"lichess_\"\n",
        "\n",
        "\n",
        "input_file = f'{DATA_DIR}{prefix}100mb.csv'\n",
        "output_file = input_file.replace(\".csv\", \"_filtered.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wInpSMZ0Sm6c"
      },
      "source": [
        "First, we download the dataset if not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRfau0wRSm6d"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(input_file):\n",
        "    dataset_path = \"adamkarvonen/chess_games\"\n",
        "    file_path = f\"{prefix}100mb.zip\"\n",
        "    # No idea why streaming=True is required to avoid an error here. Huggingface ¯\\_(ツ)_/¯\n",
        "    dataset = load_dataset(dataset_path, data_files=file_path,streaming=True)\n",
        "    df = pd.DataFrame(dataset['train'])\n",
        "    df.to_csv(input_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgWOSWLxSm6d"
      },
      "source": [
        "Our LLMs need a delimiter token \";\" at the beginning of every PGN string or it won't work as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqOprv-DSm6e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(input_file)\n",
        "\n",
        "def format_transcript(game: str) -> str:\n",
        "    new_game = ';' + game\n",
        "    return new_game\n",
        "\n",
        "df['transcript'] = df['transcript'].apply(format_transcript)\n",
        "\n",
        "for game in df.head()['transcript']:\n",
        "    print(game)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nESZdA6GSm6e"
      },
      "source": [
        "Filter all games to be len 365. This means we discard anything under that length. I chose 365 because that's the 50% of df.describe(). I also count the number of moves (with x.split()) and discard anything below the 25th percentile. This makes it easier if I want to do any move based indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkIwFk9ZSm6e"
      },
      "outputs": [],
      "source": [
        "len_df = df['transcript'].apply(lambda x: len(x))\n",
        "print(len_df.describe())\n",
        "\n",
        "game_length_in_chars = 365\n",
        "\n",
        "# Data setup. All games must have same length. 50% are >= 690 moves. I will discard all games less than 680, and truncate the rest to 680.\n",
        "filtered_df = df[df['transcript'].apply(lambda x: len(x) >= game_length_in_chars)].copy()\n",
        "filtered_df.loc[:, 'transcript'] = filtered_df['transcript'].apply(lambda x: x[:game_length_in_chars])\n",
        "\n",
        "len_df = filtered_df['transcript'].apply(lambda x: len(x))\n",
        "print(len_df.describe())\n",
        "\n",
        "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
        "move_count = move_count_df.describe()\n",
        "print(\"move count\", move_count_df.describe())\n",
        "quarter_percentile = move_count['25%']\n",
        "print(\"quarter percentile\", quarter_percentile)\n",
        "\n",
        "# Now I need to filter out games that are too short. I will discard all games less than 25th percentile  moves.\n",
        "filtered_df = filtered_df[filtered_df['transcript'].apply(lambda x: len(x.split()) >= quarter_percentile)]\n",
        "print(filtered_df.describe())\n",
        "print(filtered_df.head())\n",
        "\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
        "print(move_count_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npAGdI7kSm6f"
      },
      "outputs": [],
      "source": [
        "print(len(filtered_df))\n",
        "print(filtered_df['WhiteElo'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmo92P9GSm6f"
      },
      "source": [
        "For the classification task, I wanted some Elo bins for the probe to classify. This somewhat arbitrarily creates 6 different Elo bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcRxL-GBSm6g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Function to create binned columns and bin index columns\n",
        "def create_binned_columns(df, column_name):\n",
        "\n",
        "    # Ensure column is numeric and handle NaN values. Here, we choose to drop them, but you might fill them instead.\n",
        "    if df[column_name].dtype.kind not in 'biufc' or pd.isnull(df[column_name]).any():\n",
        "        df = df.dropna(subset=[column_name])\n",
        "        df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
        "\n",
        "    binned_column_name = f'{column_name}Binned'\n",
        "    bin_index_column_name = f'{column_name}BinIndex'\n",
        "\n",
        "    # Create quantile-based bins\n",
        "    num_bins = 6\n",
        "    # Create quantile-based bins with range labels, dropping duplicates if necessary\n",
        "    df[binned_column_name], bins = pd.qcut(df[column_name], q=num_bins, retbins=True, duplicates='drop')\n",
        "\n",
        "    # Convert bin labels to strings and assign to the column\n",
        "    df[binned_column_name] = df[binned_column_name].apply(lambda x: f'({x.left}, {x.right}]')\n",
        "\n",
        "    # Create bin index column\n",
        "    df[bin_index_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
        "\n",
        "# Apply the function to both WhiteElo and BlackElo\n",
        "create_binned_columns(filtered_df, 'WhiteElo')\n",
        "create_binned_columns(filtered_df, 'BlackElo')\n",
        "\n",
        "filtered_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
        "\n",
        "# Histogram for WhiteElo\n",
        "axes[0].hist(filtered_df['WhiteElo'], bins=30, color='blue', alpha=0.7)\n",
        "axes[0].set_title('WhiteElo Distribution')\n",
        "axes[0].set_xlabel('WhiteElo')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Bar chart for WhiteEloBinned\n",
        "bin_counts = filtered_df['WhiteEloBinned'].value_counts()\n",
        "axes[1].bar(bin_counts.index.astype(str), bin_counts.values, color='green', alpha=0.7)\n",
        "axes[1].set_title('WhiteElo Binned Distribution')\n",
        "axes[1].set_xlabel('WhiteElo Bins')\n",
        "axes[1].set_ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgMBpyETSm6g"
      },
      "outputs": [],
      "source": [
        "print(filtered_df['WhiteEloBinned'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNtBR4hkSm6h"
      },
      "outputs": [],
      "source": [
        "print(filtered_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljtkUQaqSm6j"
      },
      "outputs": [],
      "source": [
        "# shuffle all rows of the dataset\n",
        "\n",
        "df = pd.read_csv(output_file)\n",
        "df = df.sample(frac=1, random_state=200).reset_index(drop=True)\n",
        "df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut8qa5JYSm6j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(output_file)\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "# Split df into a train and test split\n",
        "train = df.sample(frac=0.5, random_state=200)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "\n",
        "# Save the train and test splits to csv\n",
        "train.to_csv(f'{DATA_DIR}{prefix}train.csv', index=False)\n",
        "test.to_csv(f'{DATA_DIR}{prefix}test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "MAX_MOVES = 15  # Limit to first 15 moves\n",
        "BOARD_SIZE = 8\n",
        "NUM_PIECE_TYPES = 12  # 6 piece types for each color\n",
        "\n",
        "# Function to convert board to tensor\n",
        "def board_to_tensor(board):\n",
        "    tensor = torch.zeros(NUM_PIECE_TYPES, BOARD_SIZE, BOARD_SIZE)\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece:\n",
        "            color = int(piece.color)\n",
        "            piece_type = piece.piece_type - 1\n",
        "            rank, file = divmod(square, 8)\n",
        "            tensor[piece_type + 6 * color][rank][file] = 1\n",
        "    return tensor\n",
        "\n",
        "# Custom dataset\n",
        "class StaffordGambitDataset(Dataset):\n",
        "    def __init__(self, games):\n",
        "        self.positions = []\n",
        "        self.moves = []\n",
        "\n",
        "        for game in games:\n",
        "            board = game.board()\n",
        "            for i, move in enumerate(game.mainline_moves()):\n",
        "                if i >= MAX_MOVES * 2:  # Both players' moves\n",
        "                    break\n",
        "                if board.turn == chess.BLACK:  # We're only interested in Black's moves\n",
        "                    self.positions.append(board_to_tensor(board))\n",
        "                    self.moves.append(move.from_square * 64 + move.to_square)\n",
        "                board.push(move)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.positions[idx], self.moves[idx]\n",
        "\n",
        "# Define the model\n",
        "class StaffordGambitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StaffordGambitModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(NUM_PIECE_TYPES, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * BOARD_SIZE * BOARD_SIZE, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 64 * 64)  # Output for all possible moves\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(-1, 256 * BOARD_SIZE * BOARD_SIZE)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load your PGN file with Stafford Gambit games\n",
        "    games = []\n",
        "    with open('stafford.txt') as pgn:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None:\n",
        "                break\n",
        "            games.append(game)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = StaffordGambitDataset(games)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = StaffordGambitModel().to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'stafford_gambit_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yADOilWkS7UP",
        "outputId": "f309e13a-e33d-4483-e2c0-4032af9a9a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 completed\n",
            "Epoch 2/10 completed\n",
            "Epoch 3/10 completed\n",
            "Epoch 4/10 completed\n",
            "Epoch 5/10 completed\n",
            "Epoch 6/10 completed\n",
            "Epoch 7/10 completed\n",
            "Epoch 8/10 completed\n",
            "Epoch 9/10 completed\n",
            "Epoch 10/10 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path, device):\n",
        "    model = StaffordGambitModel().to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vHesWOB7TfHa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom validation dataset\n",
        "class StaffordGambitValidationDataset(Dataset):\n",
        "    def __init__(self, games):\n",
        "        self.positions = []\n",
        "        self.moves = []\n",
        "\n",
        "        for game in games:\n",
        "            board = game.board()\n",
        "            for i, move in enumerate(game.mainline_moves()):\n",
        "                if i >= MAX_MOVES * 2:  # Both players' moves\n",
        "                    break\n",
        "                if board.turn == chess.BLACK:  # We're only interested in Black's moves\n",
        "                    self.positions.append(board_to_tensor(board))\n",
        "                    self.moves.append(move.from_square * 64 + move.to_square)\n",
        "                board.push(move)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.positions[idx], self.moves[idx]\n",
        "\n",
        "# Load validation games\n",
        "def load_validation_games(file_path):\n",
        "    games = []\n",
        "    with open(file_path) as pgn:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None:\n",
        "                break\n",
        "            games.append(game)\n",
        "    return games\n"
      ],
      "metadata": {
        "id": "UpfRSJNdTijq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, validation_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in validation_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            validation_loss += loss.item() * data.size(0)  # Sum the batch losses\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    validation_loss /= total  # Average loss\n",
        "    accuracy = correct / total\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "# Main validation function\n",
        "def main_validation():\n",
        "    # Load validation games\n",
        "    validation_games = load_validation_games('stafford.txt')\n",
        "\n",
        "    # Create validation dataset and dataloader\n",
        "    validation_dataset = StaffordGambitValidationDataset(validation_games)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model, criterion\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = load_model('stafford_gambit_model.pth', device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Evaluate model\n",
        "    validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_validation()\n"
      ],
      "metadata": {
        "id": "b2rCYD00Tm8q",
        "outputId": "9a91d35a-8917-42e6-83cd-bfadaaf7bea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.0788, Validation Accuracy: 0.5166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants\n",
        "MAX_MOVES = 15\n",
        "BOARD_SIZE = 8\n",
        "NUM_PIECE_TYPES = 12\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Function to convert board to tensor\n",
        "def board_to_tensor(board):\n",
        "    tensor = torch.zeros(NUM_PIECE_TYPES, BOARD_SIZE, BOARD_SIZE)\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece:\n",
        "            color = int(piece.color)\n",
        "            piece_type = piece.piece_type - 1\n",
        "            rank, file = divmod(square, 8)\n",
        "            tensor[piece_type + 6 * color][rank][file] = 1\n",
        "    return tensor\n",
        "\n",
        "# Custom dataset\n",
        "class StaffordGambitDataset(Dataset):\n",
        "    def __init__(self, games):\n",
        "        self.positions = []\n",
        "        self.moves = []\n",
        "\n",
        "        for game in games:\n",
        "            board = game.board()\n",
        "            for i, move in enumerate(game.mainline_moves()):\n",
        "                if i >= MAX_MOVES * 2:\n",
        "                    break\n",
        "                if board.turn == chess.BLACK:\n",
        "                    self.positions.append(board_to_tensor(board))\n",
        "                    self.moves.append(move.from_square * 64 + move.to_square)\n",
        "                board.push(move)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.positions[idx], self.moves[idx]\n",
        "\n",
        "# Define the model\n",
        "class StaffordGambitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StaffordGambitModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(NUM_PIECE_TYPES, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * BOARD_SIZE * BOARD_SIZE, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 64 * 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(-1, 256 * BOARD_SIZE * BOARD_SIZE)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += criterion(output, target).item() * data.size(0)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return loss / total, correct / total\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load your PGN file with Stafford Gambit games\n",
        "    games = []\n",
        "    with open('stafford.txt') as pgn:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None:\n",
        "                break\n",
        "            games.append(game)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_games, val_games = train_test_split(games, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = StaffordGambitDataset(train_games)\n",
        "    val_dataset = StaffordGambitDataset(val_games)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = StaffordGambitModel().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(model, train_loader, optimizer, criterion, device)\n",
        "        train_loss, train_accuracy = evaluate(model, train_loader, criterion, device)\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"                 - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'stafford_gambit_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aBZzLeujUCF2",
        "outputId": "0703f816-f4df-406c-d071-65d69450f199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 5.6364, Train Accuracy: 0.0984\n",
            "                 - Val Loss: 5.3306, Val Accuracy: 0.1379\n",
            "Epoch 2/20 - Train Loss: 4.3830, Train Accuracy: 0.0984\n",
            "                 - Val Loss: 4.6930, Val Accuracy: 0.1379\n",
            "Epoch 3/20 - Train Loss: 3.8586, Train Accuracy: 0.0984\n",
            "                 - Val Loss: 4.7174, Val Accuracy: 0.1379\n",
            "Epoch 4/20 - Train Loss: 3.5258, Train Accuracy: 0.1066\n",
            "                 - Val Loss: 4.8619, Val Accuracy: 0.1379\n",
            "Epoch 5/20 - Train Loss: 3.3068, Train Accuracy: 0.0984\n",
            "                 - Val Loss: 4.4309, Val Accuracy: 0.1379\n",
            "Epoch 6/20 - Train Loss: 3.1950, Train Accuracy: 0.0984\n",
            "                 - Val Loss: 4.1250, Val Accuracy: 0.1379\n",
            "Epoch 7/20 - Train Loss: 3.1110, Train Accuracy: 0.1066\n",
            "                 - Val Loss: 3.9788, Val Accuracy: 0.1379\n",
            "Epoch 8/20 - Train Loss: 3.0633, Train Accuracy: 0.1066\n",
            "                 - Val Loss: 3.9084, Val Accuracy: 0.1379\n",
            "Epoch 9/20 - Train Loss: 2.9973, Train Accuracy: 0.1066\n",
            "                 - Val Loss: 3.8493, Val Accuracy: 0.1379\n",
            "Epoch 10/20 - Train Loss: 2.9437, Train Accuracy: 0.3934\n",
            "                 - Val Loss: 3.9109, Val Accuracy: 0.4828\n",
            "Epoch 11/20 - Train Loss: 2.8624, Train Accuracy: 0.3115\n",
            "                 - Val Loss: 3.8707, Val Accuracy: 0.4138\n",
            "Epoch 12/20 - Train Loss: 2.7407, Train Accuracy: 0.1311\n",
            "                 - Val Loss: 3.8560, Val Accuracy: 0.1379\n",
            "Epoch 13/20 - Train Loss: 2.5758, Train Accuracy: 0.5246\n",
            "                 - Val Loss: 3.7350, Val Accuracy: 0.6207\n",
            "Epoch 14/20 - Train Loss: 2.3815, Train Accuracy: 0.4918\n",
            "                 - Val Loss: 3.5360, Val Accuracy: 0.6207\n",
            "Epoch 15/20 - Train Loss: 2.1358, Train Accuracy: 0.5246\n",
            "                 - Val Loss: 3.4780, Val Accuracy: 0.6207\n",
            "Epoch 16/20 - Train Loss: 1.9217, Train Accuracy: 0.4426\n",
            "                 - Val Loss: 3.3723, Val Accuracy: 0.5172\n",
            "Epoch 17/20 - Train Loss: 1.7213, Train Accuracy: 0.5410\n",
            "                 - Val Loss: 3.1456, Val Accuracy: 0.6207\n",
            "Epoch 18/20 - Train Loss: 1.4915, Train Accuracy: 0.5902\n",
            "                 - Val Loss: 3.2966, Val Accuracy: 0.6552\n",
            "Epoch 19/20 - Train Loss: 1.3016, Train Accuracy: 0.6066\n",
            "                 - Val Loss: 3.9234, Val Accuracy: 0.6897\n",
            "Epoch 20/20 - Train Loss: 1.1187, Train Accuracy: 0.6803\n",
            "                 - Val Loss: 4.0937, Val Accuracy: 0.6897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants\n",
        "MAX_MOVES = 15\n",
        "BOARD_SIZE = 8\n",
        "NUM_PIECE_TYPES = 12\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Function to convert board to tensor\n",
        "def board_to_tensor(board):\n",
        "    tensor = torch.zeros(NUM_PIECE_TYPES, BOARD_SIZE, BOARD_SIZE)\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece:\n",
        "            color = int(piece.color)\n",
        "            piece_type = piece.piece_type - 1\n",
        "            rank, file = divmod(square, 8)\n",
        "            tensor[piece_type + 6 * color][rank][file] = 1\n",
        "    return tensor\n",
        "\n",
        "# Custom dataset\n",
        "class StaffordGambitDataset(Dataset):\n",
        "    def __init__(self, games):\n",
        "        self.positions = []\n",
        "        self.moves = []\n",
        "\n",
        "        for game in games:\n",
        "            board = game.board()\n",
        "            for i, move in enumerate(game.mainline_moves()):\n",
        "                if i >= MAX_MOVES * 2:\n",
        "                    break\n",
        "                if board.turn == chess.BLACK:\n",
        "                    self.positions.append(board_to_tensor(board))\n",
        "                    self.moves.append(move.from_square * 64 + move.to_square)\n",
        "                board.push(move)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.positions[idx], self.moves[idx]\n",
        "\n",
        "# Define the model with dropout and batch normalization\n",
        "class StaffordGambitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StaffordGambitModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(NUM_PIECE_TYPES, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(256 * BOARD_SIZE * BOARD_SIZE, 1024)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 64 * 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * BOARD_SIZE * BOARD_SIZE)\n",
        "        x = torch.relu(self.dropout(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += criterion(output, target).item() * data.size(0)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return loss / total, correct / total\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load your PGN file with Stafford Gambit games\n",
        "    games = []\n",
        "    with open('stafford.txt') as pgn:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None:\n",
        "                break\n",
        "            games.append(game)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_games, val_games = train_test_split(games, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = StaffordGambitDataset(train_games)\n",
        "    val_dataset = StaffordGambitDataset(val_games)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = StaffordGambitModel().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(model, train_loader, optimizer, criterion, device)\n",
        "        train_loss, train_accuracy = evaluate(model, train_loader, criterion, device)\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"                 - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'stafford_gambit_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "M_4S6lkfUlNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "othello",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}